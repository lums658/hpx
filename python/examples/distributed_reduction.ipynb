{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPXPy Distributed Reduction Demo\n",
    "\n",
    "This notebook demonstrates how collective operations enable distributed computing patterns using the **SPMD (Single Program, Multiple Data)** execution model.\n",
    "\n",
    "In single-locality mode (like this notebook), we demonstrate the API and pattern. In multi-locality mode, this would run across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "hpx.init(num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_localities = hpx.collectives.get_num_localities()\n",
    "locality_id = hpx.collectives.get_locality_id()\n",
    "\n",
    "print(f\"Locality Configuration:\")\n",
    "print(f\"  Number of localities: {num_localities}\")\n",
    "print(f\"  This locality ID: {locality_id}\")\n",
    "print(f\"  HPX threads: {hpx.num_threads()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Distributed Sum (All-Reduce)\n",
    "\n",
    "In a real distributed scenario:\n",
    "- Each locality would have a different portion of the data\n",
    "- `all_reduce` combines all local sums into a global sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate local data (in multi-locality, each would have different data)\n",
    "np.random.seed(42 + locality_id)  # Different seed per locality\n",
    "local_data = np.random.randn(1000000)\n",
    "local_arr = hpx.from_numpy(local_data)\n",
    "\n",
    "# Compute local sum\n",
    "local_sum = float(hpx.sum(local_arr))\n",
    "print(f\"Local sum (locality {locality_id}): {local_sum:.4f}\")\n",
    "\n",
    "# All-reduce to get global sum\n",
    "local_sum_arr = hpx.from_numpy(np.array([local_sum]))\n",
    "global_sum_arr = hpx.all_reduce(local_sum_arr, op='sum')\n",
    "global_sum = float(global_sum_arr.to_numpy()[0])\n",
    "\n",
    "print(f\"Global sum (all localities): {global_sum:.4f}\")\n",
    "\n",
    "if num_localities == 1:\n",
    "    print(\"\\nNote: In single-locality mode, global_sum == local_sum\")\n",
    "    print(\"With N localities, this would sum contributions from all N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Parameter Broadcast\n",
    "\n",
    "Root locality computes parameters, then broadcasts to all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if locality_id == 0:\n",
    "    # Only root does this computation\n",
    "    params = np.array([0.01, 0.99, 42.0])  # learning_rate, momentum, seed\n",
    "    print(f\"Root computed parameters: {params}\")\n",
    "else:\n",
    "    params = np.zeros(3)  # Other localities wait for broadcast\n",
    "\n",
    "params_arr = hpx.from_numpy(params)\n",
    "params_arr = hpx.broadcast(params_arr, root=0)\n",
    "received_params = params_arr.to_numpy()\n",
    "\n",
    "print(f\"Locality {locality_id} received: {received_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Gather Local Statistics\n",
    "\n",
    "Each locality computes local statistics, then gathers them to root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each locality computes local statistics\n",
    "local_mean = float(hpx.mean(local_arr))\n",
    "local_std = float(hpx.std(local_arr))\n",
    "local_stats = np.array([local_mean, local_std])\n",
    "\n",
    "print(f\"Locality {locality_id} stats: mean={local_mean:.4f}, std={local_std:.4f}\")\n",
    "\n",
    "local_stats_arr = hpx.from_numpy(local_stats)\n",
    "all_stats = hpx.gather(local_stats_arr, root=0)\n",
    "\n",
    "if locality_id == 0:\n",
    "    print(f\"Root gathered {len(all_stats)} locality stats:\")\n",
    "    for i, stats in enumerate(all_stats):\n",
    "        print(f\"  From locality {i}: mean={stats[0]:.4f}, std={stats[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: Barrier Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Locality {locality_id} reaching barrier...\")\n",
    "start = time.perf_counter()\n",
    "hpx.barrier(\"demo_barrier\")\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Locality {locality_id} passed barrier in {elapsed*1000:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Pattern\n",
    "\n",
    "The SPMD pattern demonstrated above enables:\n",
    "\n",
    "### 1. Data Parallelism\n",
    "\n",
    "```\n",
    "┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐\n",
    "│ Locality 0  │  │ Locality 1  │  │ Locality 2  │  │ Locality 3  │\n",
    "│ Data[0:N/4] │  │ Data[N/4:N/2│  │Data[N/2:3N/4│  │Data[3N/4:N] │\n",
    "└─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘\n",
    "      ↓                ↓                ↓                ↓\n",
    "┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐\n",
    "│ Local Comp  │  │ Local Comp  │  │ Local Comp  │  │ Local Comp  │\n",
    "└─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘\n",
    "      ↓                ↓                ↓                ↓\n",
    "┌──────────────────────────────────────────────────────────────────┐\n",
    "│                        ALL-REDUCE                                │\n",
    "│                  Combine local results                           │\n",
    "└──────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2. Collective Operations\n",
    "\n",
    "| Operation | Description |\n",
    "|-----------|-------------|\n",
    "| `all_reduce` | Combine values, result on all localities |\n",
    "| `broadcast` | Send from one to all |\n",
    "| `gather` | Collect from all to one |\n",
    "| `scatter` | Distribute from one to all |\n",
    "| `barrier` | Synchronize all localities |\n",
    "\n",
    "### 3. Use Cases\n",
    "\n",
    "- **Machine Learning**: Distributed gradient descent\n",
    "- **Scientific Computing**: Domain decomposition\n",
    "- **Data Analytics**: MapReduce patterns\n",
    "- **Simulation**: Parallel time stepping\n",
    "\n",
    "### Running Multi-Locality (future)\n",
    "\n",
    "```bash\n",
    "mpirun -n 4 python script.py --hpx:threads=8\n",
    "# or\n",
    "srun -n 4 python script.py --hpx:threads=8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx.finalize()\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
