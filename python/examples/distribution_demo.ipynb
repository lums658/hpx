{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Data Distribution with PartitionedArray\n\nThis notebook demonstrates how HPXPy distributes data across multiple localities using `PartitionedArray` (backed by `hpx::partitioned_vector`).\n\nKey concepts:\n- **Partitions**: Contiguous chunks of data, each residing on one locality\n- **Distribution**: How partitions are assigned to localities\n- **Distributed operations**: Element-wise and reduction operations that work across partitions\n\nWhen running with N localities, each locality physically owns a portion of the data. Operations execute locally on each partition and combine results as needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile _distribution_demo_worker.py\n\"\"\"Data distribution demo: how PartitionedArray partitions data across localities.\"\"\"\nimport sys\nimport numpy as np\nimport hpxpy as hpx\nfrom hpxpy.launcher import init_from_args\n\ninit_from_args()\n\nmy_id = hpx.locality_id()\nnum_locs = hpx.num_localities()\nprint(f\"[Locality {my_id}/{num_locs}] Started with {hpx.num_threads()} threads\")\n\n# ============================================================\n# 1. Creating Distributed Arrays\n# ============================================================\nprint(f\"\\n{'='*50}\")\nprint(\"1. PartitionedArray Creation\")\nprint(f\"{'='*50}\")\n\n# Create arrays using different factory functions\narr_arange = hpx.partitioned_arange(0, 20)\narr_zeros = hpx.partitioned_zeros(10)\narr_ones = hpx.partitioned_ones(10)\narr_full = hpx.partitioned_full(10, 3.14)\narr_numpy = hpx.partitioned_from_numpy(np.array([10.0, 20.0, 30.0, 40.0, 50.0]))\n\nif my_id == 0:\n    print(f\"  partitioned_arange(0, 20): {arr_arange.num_partitions} partitions, distributed={arr_arange.is_distributed}\")\n    print(f\"  partitioned_zeros(10):     {arr_zeros.num_partitions} partitions, distributed={arr_zeros.is_distributed}\")\n    print(f\"  partitioned_ones(10):      {arr_ones.num_partitions} partitions, distributed={arr_ones.is_distributed}\")\n    print(f\"  partitioned_full(10, pi):  {arr_full.num_partitions} partitions, distributed={arr_full.is_distributed}\")\n    print(f\"  partitioned_from_numpy:    {arr_numpy.num_partitions} partitions, distributed={arr_numpy.is_distributed}\")\n\nhpx.barrier(\"after_creation\")\n\n# ============================================================\n# 2. Distributed Reductions\n# ============================================================\nprint(f\"\\n{'='*50}\")\nprint(\"2. Distributed Reductions\")\nprint(f\"{'='*50}\")\n\n# Create a larger distributed array for meaningful reductions\ndata = hpx.partitioned_from_numpy(np.arange(1000, dtype=np.float64))\n\nif my_id == 0:\n    print(f\"  Array: partitioned_from_numpy(arange(1000))\")\n    print(f\"  Partitions: {data.num_partitions}\")\n    print(f\"  Distributed: {data.is_distributed}\")\n    print()\n\n# All reductions work across all localities\ns = hpx.distributed_sum(data)\nm = hpx.distributed_mean(data)\nmn = hpx.distributed_min(data)\nmx = hpx.distributed_max(data)\nv = hpx.distributed_var(data)\nsd = hpx.distributed_std(data)\n\nif my_id == 0:\n    print(f\"  sum  = {s}\")\n    print(f\"  mean = {m}\")\n    print(f\"  min  = {mn}\")\n    print(f\"  max  = {mx}\")\n    print(f\"  var  = {v:.4f}\")\n    print(f\"  std  = {sd:.4f}\")\n\n    # Verify against NumPy\n    ref = np.arange(1000, dtype=np.float64)\n    assert s == np.sum(ref), f\"sum mismatch: {s} vs {np.sum(ref)}\"\n    assert m == np.mean(ref), f\"mean mismatch: {m} vs {np.mean(ref)}\"\n    assert mn == np.min(ref), f\"min mismatch: {mn} vs {np.min(ref)}\"\n    assert mx == np.max(ref), f\"max mismatch: {mx} vs {np.max(ref)}\"\n    print(\"  (All verified against NumPy)\")\n\nhpx.barrier(\"after_reductions\")\n\n# ============================================================\n# 3. Converting Back to NumPy\n# ============================================================\nprint(f\"\\n{'='*50}\")\nprint(\"3. Conversion to NumPy\")\nprint(f\"{'='*50}\")\n\nsmall = hpx.partitioned_from_numpy(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\nlocal_copy = small.to_numpy()\nif my_id == 0:\n    print(f\"  PartitionedArray → NumPy: {local_copy}\")\n    print(f\"  Type: {type(local_copy)}\")\n\nhpx.barrier(\"done\")\nif my_id == 0:\n    print(\"\\nAll localities completed successfully.\")\nhpx.finalize()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Launch Distributed Execution\n\nThe worker script runs on multiple localities. PartitionedArrays are automatically split across the localities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from hpxpy.launcher import launch_localities\n\nlaunch_localities(\n    \"_distribution_demo_worker.py\",\n    num_localities=2,\n    threads_per_locality=2,\n    verbose=True,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How Partitioning Works\n\nWhen `partitioned_from_numpy(data)` is called with N localities:\n\n```\nNumPy array: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n                    ┌─── partitioned_from_numpy ───┐\n                    ▼                               ▼\n         Locality 0                      Locality 1\n    ┌──────────────────┐           ┌──────────────────┐\n    │ Partition 0      │           │ Partition 1      │\n    │ [0, 1, 2, 3, 4] │           │ [5, 6, 7, 8, 9]  │\n    └──────────────────┘           └──────────────────┘\n```\n\n### Distributed Reduction Flow\n\n```\n    Locality 0              Locality 1\n    sum([0..4]) = 10        sum([5..9]) = 35\n         │                       │\n         └───── combine ─────────┘\n                    │\n              global sum = 45\n```\n\nThis happens automatically when you call `distributed_sum()`, `distributed_mean()`, etc.\n\n### API Summary\n\n| Creation | Description |\n|----------|-------------|\n| `partitioned_zeros(n)` | Distributed zeros |\n| `partitioned_ones(n)` | Distributed ones |\n| `partitioned_full(n, val)` | Distributed fill |\n| `partitioned_arange(start, stop)` | Distributed range |\n| `partitioned_from_numpy(arr)` | Distribute existing data |\n\n| Property | Description |\n|----------|-------------|\n| `.num_partitions` | Number of data partitions |\n| `.is_distributed` | True if multi-locality |\n| `.to_numpy()` | Gather all data to local NumPy array |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos.remove(\"_distribution_demo_worker.py\")\nprint(\"Cleaned up worker script.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}