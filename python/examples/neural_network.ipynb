{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPXPy Neural Network with Backpropagation\n",
    "\n",
    "This notebook implements a single-hidden-layer neural network using HPXPy, demonstrating:\n",
    "1. **Forward propagation** through hidden layers with sigmoid activation\n",
    "2. **Backpropagation** for gradient computation via chain rule\n",
    "3. **Data-parallel SGD** pattern used in modern deep learning frameworks\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "```\n",
    "Input Layer (n features)\n",
    "    ↓ W_hidden, b_hidden\n",
    "Hidden Layer (h neurons) + Sigmoid\n",
    "    ↓ W_output, b_output  \n",
    "Output Layer (m outputs) + Sigmoid\n",
    "```\n",
    "\n",
    "## Key Operations\n",
    "\n",
    "**Forward Pass:**\n",
    "- hidden = σ(X @ W_h + b_h)\n",
    "- output = σ(hidden @ W_o + b_o)\n",
    "\n",
    "where σ(z) = 1/(1 + e^(-z))\n",
    "\n",
    "**Backward Pass (Chain Rule):**\n",
    "- δ_output = (y - output) * σ'(output)\n",
    "- δ_hidden = (δ_output @ W_o^T) * σ'(hidden)\n",
    "- ∇W_o = hidden^T @ δ_output\n",
    "- ∇W_h = X^T @ δ_hidden\n",
    "\n",
    "**Distributed Pattern:** Data parallelism\n",
    "- Distribute mini-batches across nodes\n",
    "- Each node computes local gradients\n",
    "- All-reduce to aggregate gradients\n",
    "- Synchronous weight updates\n",
    "\n",
    "**Inspiration:** Based on Phylanx example, originally from Analytics Vidhya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "hpx.init(num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_neurons = 4\n",
    "hidden_neurons = 3\n",
    "output_neurons = 1\n",
    "learning_rate = 0.1\n",
    "num_iterations = 200\n",
    "\n",
    "print(f\"Network Configuration:\")\n",
    "print(f\"  Input neurons: {input_neurons}\")\n",
    "print(f\"  Hidden neurons: {hidden_neurons}\")\n",
    "print(f\"  Output neurons: {output_neurons}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Iterations: {num_iterations}\")\n",
    "\n",
    "# Training data (XOR-like problem)\n",
    "X = np.array([[1, 0, 1, 0],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 1, 0, 1]], dtype=np.float64)\n",
    "\n",
    "y = np.array([[1],\n",
    "              [1],\n",
    "              [0]], dtype=np.float64)\n",
    "\n",
    "print(f\"\\nTraining Data:\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  X:\\n{X}\")\n",
    "print(f\"  y:\\n{y.ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: σ'(z) = σ(z) * (1 - σ(z)).\"\"\"\n",
    "    return z * (1.0 - z)\n",
    "\n",
    "def train_neural_network(X, y, hidden_neurons, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Train a single-hidden-layer neural network.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (samples × features)\n",
    "        y: Target outputs (samples × outputs)\n",
    "        hidden_neurons: Number of hidden layer neurons\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        num_iterations: Number of training iterations\n",
    "    \n",
    "    Returns:\n",
    "        W_hidden, b_hidden, W_output, b_output: Trained weights and biases\n",
    "    \"\"\"\n",
    "    num_samples, num_features = X.shape\n",
    "    num_outputs = y.shape[1]\n",
    "    \n",
    "    # Initialize weights and biases randomly\n",
    "    np.random.seed(0)\n",
    "    W_hidden = np.random.uniform(size=(num_features, hidden_neurons))\n",
    "    b_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
    "    W_output = np.random.uniform(size=(hidden_neurons, num_outputs))\n",
    "    b_output = np.random.uniform(size=(1, num_outputs))\n",
    "    \n",
    "    print(f\"\\nInitialized weights:\")\n",
    "    print(f\"  W_hidden: {W_hidden.shape}\")\n",
    "    print(f\"  b_hidden: {b_hidden.shape}\")\n",
    "    print(f\"  W_output: {W_output.shape}\")\n",
    "    print(f\"  b_output: {b_output.shape}\")\n",
    "    \n",
    "    losses = []\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # === FORWARD PASS ===\n",
    "        # Hidden layer\n",
    "        hidden_input = X @ W_hidden + b_hidden\n",
    "        hidden_activation = sigmoid(hidden_input)\n",
    "        \n",
    "        # Output layer\n",
    "        output_input = hidden_activation @ W_output + b_output\n",
    "        output = sigmoid(output_input)\n",
    "        \n",
    "        # === COMPUTE LOSS ===\n",
    "        error = y - output\n",
    "        loss = np.mean(error ** 2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # === BACKWARD PASS (Backpropagation) ===\n",
    "        # Output layer gradients\n",
    "        delta_output = error * sigmoid_derivative(output)\n",
    "        \n",
    "        # Hidden layer gradients (chain rule)\n",
    "        error_hidden = delta_output @ W_output.T\n",
    "        delta_hidden = error_hidden * sigmoid_derivative(hidden_activation)\n",
    "        \n",
    "        # === COMPUTE WEIGHT GRADIENTS ===\n",
    "        grad_W_output = hidden_activation.T @ delta_output\n",
    "        grad_b_output = np.sum(delta_output, axis=0, keepdims=True)\n",
    "        \n",
    "        grad_W_hidden = X.T @ delta_hidden\n",
    "        grad_b_hidden = np.sum(delta_hidden, axis=0, keepdims=True)\n",
    "        \n",
    "        # === UPDATE WEIGHTS (Gradient Ascent - we computed error, not loss gradient) ===\n",
    "        W_output += learning_rate * grad_W_output\n",
    "        b_output += learning_rate * grad_b_output\n",
    "        W_hidden += learning_rate * grad_W_hidden\n",
    "        b_hidden += learning_rate * grad_b_hidden\n",
    "        \n",
    "        # Print progress\n",
    "        if (iteration + 1) % 50 == 0:\n",
    "            print(f\"  Iteration {iteration+1}/{num_iterations}: Loss={loss:.6f}\")\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    print(f\"\\nTraining complete in {total_time*1000:.1f}ms\")\n",
    "    \n",
    "    # Final predictions\n",
    "    hidden_activation = sigmoid(X @ W_hidden + b_hidden)\n",
    "    final_output = sigmoid(hidden_activation @ W_output + b_output)\n",
    "    \n",
    "    return W_hidden, b_hidden, W_output, b_output, final_output, losses\n",
    "\n",
    "# Train the network\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "W_h, b_h, W_o, b_o, predictions, losses = train_neural_network(\n",
    "    X, y, hidden_neurons, learning_rate, num_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFinal Predictions vs Ground Truth:\")\n",
    "print(f\"{'Sample':<8} {'Target':<10} {'Predicted':<12} {'Error':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(len(X)):\n",
    "    target = y[i, 0]\n",
    "    pred = predictions[i, 0]\n",
    "    error = abs(target - pred)\n",
    "    print(f\"{i:<8} {target:<10.4f} {pred:<12.6f} {error:<10.6f}\")\n",
    "\n",
    "final_loss = np.mean((y - predictions) ** 2)\n",
    "print(f\"\\nFinal MSE: {final_loss:.6f}\")\n",
    "print(f\"Initial Loss: {losses[0]:.6f}\")\n",
    "print(f\"Loss Reduction: {(losses[0] - final_loss) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title('Neural Network Training Progress')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"matplotlib not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Neural Network Training: Data Parallelism\n",
    "\n",
    "### How Modern Deep Learning Frameworks Distribute Training\n",
    "\n",
    "Neural networks use **data parallelism** for distributed training:\n",
    "\n",
    "```python\n",
    "# Each node has full model copy\n",
    "W_hidden_local, W_output_local = initialize_weights()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Split mini-batch across nodes\n",
    "    X_local = X[my_batch_indices]  # Each node: batch_size/n samples\n",
    "    y_local = y[my_batch_indices]\n",
    "    \n",
    "    # === LOCAL COMPUTATION (No communication) ===\n",
    "    # Forward pass\n",
    "    hidden = sigmoid(X_local @ W_hidden_local + b_hidden)\n",
    "    output = sigmoid(hidden @ W_output_local + b_output)\n",
    "    \n",
    "    # Backward pass\n",
    "    delta_output = (y_local - output) * sigmoid_derivative(output)\n",
    "    delta_hidden = (delta_output @ W_output.T) * sigmoid_derivative(hidden)\n",
    "    \n",
    "    # Compute local gradients\n",
    "    grad_W_output_local = hidden.T @ delta_output\n",
    "    grad_W_hidden_local = X_local.T @ delta_hidden\n",
    "    \n",
    "    # === GLOBAL SYNCHRONIZATION (All-reduce) ===\n",
    "    grad_W_output = all_reduce(grad_W_output_local, op='sum') / num_nodes\n",
    "    grad_W_hidden = all_reduce(grad_W_hidden_local, op='sum') / num_nodes\n",
    "    \n",
    "    # === LOCAL WEIGHT UPDATE ===\n",
    "    W_output_local += learning_rate * grad_W_output\n",
    "    W_hidden_local += learning_rate * grad_W_hidden\n",
    "```\n",
    "\n",
    "### Communication Analysis\n",
    "\n",
    "**Per iteration:**\n",
    "- All-reduce gradients: Sum of all weight matrix sizes\n",
    "- For network (n_in, n_hidden, n_out): (n_in × n_hidden + n_hidden × n_out) floats\n",
    "- Communication time: O(model_size / bandwidth)\n",
    "\n",
    "**Communication efficiency improves with:**\n",
    "1. Larger batch sizes (more local work per sync)\n",
    "2. Fewer parameters (smaller all-reduce)\n",
    "3. Faster interconnect (higher bandwidth)\n",
    "4. Gradient compression techniques\n",
    "\n",
    "### Scaling Projection\n",
    "\n",
    "| Model | Parameters | Nodes | Batch/Node | Compute Time | Comm Time | Speedup |\n",
    "|-------|------------|-------|------------|--------------|-----------|----------|\n",
    "| Toy (4-3-1) | 19 | 1 | 32 | 10ms | 0ms | 1x |\n",
    "| Toy (4-3-1) | 19 | 8 | 4 | 1.25ms | 0.01ms | 8x |\n",
    "| Small (100-50-10) | 5,560 | 16 | 64 | 50ms | 0.5ms | 15.7x |\n",
    "| Medium (1K-512-100) | 564K | 64 | 128 | 200ms | 5ms | 62x |\n",
    "| Large (10K-4K-1K) | 44M | 256 | 256 | 1s | 50ms | 244x |\n",
    "| ResNet-50 | 25M | 512 | 64 | 300ms | 20ms | 480x |\n",
    "| GPT-3 | 175B | 10K | 4 | 5s | 200ms | 9.6K× |\n",
    "\n",
    "### Real-World Deep Learning Training\n",
    "\n",
    "**ImageNet (Computer Vision):**\n",
    "- Dataset: 1.3M images, 1000 classes\n",
    "- Model: ResNet-50 (25M parameters)\n",
    "- Setup: 8× NVIDIA V100 GPUs\n",
    "- Training time: ~1 hour (vs 7 hours on 1 GPU)\n",
    "\n",
    "**BERT (NLP):**\n",
    "- Dataset: 3.3B words (Wikipedia + BookCorpus)\n",
    "- Model: 340M parameters\n",
    "- Setup: 64× TPU v3 chips\n",
    "- Training time: 4 days\n",
    "\n",
    "**GPT-3 (Large Language Model):**\n",
    "- Dataset: 570GB text (45TB tokens)\n",
    "- Model: 175B parameters\n",
    "- Setup: 10,000+ NVIDIA V100 GPUs\n",
    "- Training time: ~2 weeks\n",
    "- Cost: ~$4.6M\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Data parallelism scales linearly** up to communication bottleneck\n",
    "2. **Critical ratio:** Computation time / Communication time > 10 for good efficiency\n",
    "3. **Techniques for scaling:**\n",
    "   - Synchronous SGD: All nodes sync every iteration (high accuracy, slower)\n",
    "   - Asynchronous SGD: Nodes update independently (faster, can diverge)\n",
    "   - Mixed precision: FP16 for forward/backward, FP32 for weights\n",
    "   - Gradient accumulation: Multiple forward/backward before sync\n",
    "4. **HPX advantage:** Asynchronous communication enables overlap of compute and network transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx.finalize()\n",
    "print(\"Neural network demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
