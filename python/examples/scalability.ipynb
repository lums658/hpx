{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPXPy Scalability Demonstration\n",
    "\n",
    "This notebook demonstrates HPXPy's parallel scalability through benchmarks on different workloads.\n",
    "\n",
    "**Note:** For proper thread scaling tests (1, 2, 4, 8 threads), run the `scalability_demo.py` script instead, which spawns separate processes with different HPX thread counts.\n",
    "\n",
    "## What We Test\n",
    "\n",
    "1. **Monte Carlo Pi** - Mixed workload: random generation, operators, comparison, reduction\n",
    "2. **Element-wise Operations** - Pure compute: sqrt, exp, sin, power\n",
    "3. **Reductions** - Memory-bound: sum, prod, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "hpx.init(num_threads=4)\n",
    "print(f\"Running with {hpx.num_threads()} HPX threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Monte Carlo Pi Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_pi_hpxpy(n_samples):\n",
    "    \"\"\"Monte Carlo Pi - demonstrates operators, random, reduction.\"\"\"\n",
    "    hpx.random.seed(42)\n",
    "    \n",
    "    x = hpx.random.uniform(0, 1, size=n_samples)\n",
    "    y = hpx.random.uniform(0, 1, size=n_samples)\n",
    "    distances_squared = x**2 + y**2\n",
    "    inside_mask = distances_squared <= 1\n",
    "    inside_float = hpx.from_numpy(inside_mask.to_numpy().astype(float), copy=True)\n",
    "    inside_count = hpx.sum(inside_float)\n",
    "    return 4 * inside_count / n_samples\n",
    "\n",
    "def monte_carlo_pi_numpy(n_samples):\n",
    "    \"\"\"NumPy reference.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    x = np.random.uniform(0, 1, n_samples)\n",
    "    y = np.random.uniform(0, 1, n_samples)\n",
    "    inside = np.sum(x**2 + y**2 <= 1)\n",
    "    return 4 * inside / n_samples\n",
    "\n",
    "n_samples = 50_000_000\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Monte Carlo Pi Estimation ({n_samples:,} samples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Warm up\n",
    "_ = monte_carlo_pi_hpxpy(1000)\n",
    "\n",
    "# NumPy\n",
    "start = time.perf_counter()\n",
    "pi_np = monte_carlo_pi_numpy(n_samples)\n",
    "np_time = time.perf_counter() - start\n",
    "\n",
    "# HPXPy\n",
    "start = time.perf_counter()\n",
    "pi_hpx = monte_carlo_pi_hpxpy(n_samples)\n",
    "hpx_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\n{'Method':>10} | {'Time (s)':>10} | {'Pi Estimate':>12}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'NumPy':>10} | {np_time:>10.4f} | {pi_np:>12.8f}\")\n",
    "print(f\"{'HPXPy':>10} | {hpx_time:>10.4f} | {pi_hpx:>12.8f}\")\n",
    "print(f\"\\nSpeedup: {np_time/hpx_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elements = 100_000_000\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Element-wise Operations ({n_elements:,} elements)\")\n",
    "print(\"Operations: sqrt, exp, sin, power, sum\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data\n",
    "np_arr = np.arange(n_elements, dtype=np.float64)\n",
    "hpx_arr = hpx.from_numpy(np_arr)\n",
    "\n",
    "# NumPy\n",
    "start = time.perf_counter()\n",
    "result = np.sqrt(np_arr + 1)\n",
    "result = np.exp(result * 0.001)\n",
    "result = np.sin(result)\n",
    "result = result ** 2\n",
    "_ = np.sum(result)\n",
    "np_time = time.perf_counter() - start\n",
    "\n",
    "# HPXPy\n",
    "start = time.perf_counter()\n",
    "result = hpx.sqrt(hpx_arr + 1)\n",
    "result = hpx.exp(result * 0.001)\n",
    "result = hpx.sin(result)\n",
    "result = result ** 2\n",
    "_ = hpx.sum(result)\n",
    "hpx_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nNumPy:  {np_time:.4f} s\")\n",
    "print(f\"HPXPy:  {hpx_time:.4f} s\")\n",
    "print(f\"Speedup: {np_time/hpx_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elements = 50_000_000\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reduction Operations ({n_elements:,} elements)\")\n",
    "print(\"Operations: sum, prod (small), min, max\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data\n",
    "np_arr = np.random.randn(n_elements)\n",
    "hpx_arr = hpx.from_numpy(np_arr)\n",
    "\n",
    "n_iterations = 10\n",
    "\n",
    "# NumPy\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iterations):\n",
    "    _ = np.sum(np_arr)\n",
    "    _ = np.prod(np_arr[:1000])  # Small to avoid overflow\n",
    "    _ = np.min(np_arr)\n",
    "    _ = np.max(np_arr)\n",
    "np_time = time.perf_counter() - start\n",
    "\n",
    "# HPXPy\n",
    "small_hpx = hpx.from_numpy(np_arr[:1000])\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iterations):\n",
    "    _ = hpx.sum(hpx_arr)\n",
    "    _ = hpx.prod(small_hpx)\n",
    "    _ = hpx.min(hpx_arr)\n",
    "    _ = hpx.max(hpx_arr)\n",
    "hpx_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nNumPy:  {np_time:.4f} s ({n_iterations} iterations)\")\n",
    "print(f\"HPXPy:  {hpx_time:.4f} s ({n_iterations} iterations)\")\n",
    "print(f\"Speedup: {np_time/hpx_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "HPXPy uses HPX's parallel execution policies which automatically distribute work across available threads. The speedup depends on:\n",
    "\n",
    "1. **Array size** - Larger arrays benefit more from parallelism\n",
    "2. **Operation type** - Compute-intensive operations scale better\n",
    "3. **Memory bandwidth** - Some operations are memory-bound\n",
    "\n",
    "### Key Performance Factors\n",
    "\n",
    "| Factor | Impact |\n",
    "|--------|--------|\n",
    "| SIMD vectorization | Significant speedup on element-wise ops |\n",
    "| GIL release | Python threads can execute C++ in parallel |\n",
    "| HPX parallel policies | Work stealing and load balancing |\n",
    "| Memory locality | Better cache utilization |\n",
    "\n",
    "### Distributed Parallelism\n",
    "\n",
    "Distributed parallelism (multiple processes/nodes) with AGAS-backed distributed arrays extends these benefits across multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx.finalize()\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
