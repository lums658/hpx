{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Distributed Sensor Analytics with PartitionedArray\n\nThis notebook demonstrates distributed data analytics using HPXPy's `PartitionedArray`, which distributes data across multiple HPX localities (processes) using `hpx::partitioned_vector`.\n\nWhen run with multiple localities, data is physically partitioned across processes. Each locality holds a portion of the array and performs local computation. Distributed reductions (sum, mean, min, max, var, std) automatically combine results across all localities.\n\n**To run:** Execute all cells. The worker script is launched across multiple localities automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile _distributed_analytics_worker.py\n\"\"\"Distributed sensor analytics using PartitionedArray.\n\nThis script runs on each HPX locality. The PartitionedArray distributes\ndata across all localities automatically.\n\"\"\"\nimport sys\nimport numpy as np\nimport hpxpy as hpx\nfrom hpxpy.launcher import init_from_args\n\n# Initialize HPX with launcher-provided configuration\ninit_from_args()\n\nmy_id = hpx.locality_id()\nnum_locs = hpx.num_localities()\nprint(f\"[Locality {my_id}/{num_locs}] Started with {hpx.num_threads()} threads\")\n\n# --- Create distributed sensor data ---\nn_sensors = 100_000\n\n# PartitionedArray distributes data across localities\ntemperatures = hpx.partitioned_from_numpy(\n    20.0 + np.random.RandomState(42).randn(n_sensors) * 5.0\n)\nhumidity = hpx.partitioned_from_numpy(\n    60.0 + np.random.RandomState(43).randn(n_sensors) * 10.0\n)\n\nprint(f\"[Locality {my_id}] Created PartitionedArrays:\")\nprint(f\"  Temperature: {temperatures.num_partitions} partitions, distributed={temperatures.is_distributed}\")\nprint(f\"  Humidity:    {humidity.num_partitions} partitions, distributed={humidity.is_distributed}\")\n\n# --- Distributed reductions ---\n# These automatically combine across all localities\ntemp_sum = hpx.distributed_sum(temperatures)\ntemp_mean = hpx.distributed_mean(temperatures)\ntemp_min = hpx.distributed_min(temperatures)\ntemp_max = hpx.distributed_max(temperatures)\ntemp_std = hpx.distributed_std(temperatures)\n\nhumid_mean = hpx.distributed_mean(humidity)\nhumid_std = hpx.distributed_std(humidity)\n\nif my_id == 0:\n    print(f\"\\n--- Distributed Reduction Results (computed across {num_locs} localities) ---\")\n    print(f\"  Temperature: mean={temp_mean:.2f}, std={temp_std:.2f}, min={temp_min:.2f}, max={temp_max:.2f}\")\n    print(f\"  Humidity:    mean={humid_mean:.2f}, std={humid_std:.2f}\")\n    print(f\"  Total sensor readings: {n_sensors:,}\")\n\n# Synchronize before exit\nhpx.barrier(\"done\")\nif my_id == 0:\n    print(\"\\nAll localities completed successfully.\")\nhpx.finalize()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Launch Distributed Execution\n\nThe cell below spawns multiple HPX localities (processes) connected via TCP. Each locality runs the worker script above. Data created with `partitioned_from_numpy` is automatically distributed across all localities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from hpxpy.launcher import launch_localities\n\nlaunch_localities(\n    \"_distributed_analytics_worker.py\",\n    num_localities=2,\n    threads_per_locality=2,\n    verbose=True,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How PartitionedArray Works\n\n`PartitionedArray` wraps HPX's `hpx::partitioned_vector`, which physically distributes data across localities:\n\n```\nLocality 0                    Locality 1\n┌────────────────────┐       ┌────────────────────┐\n│ Partition 0        │       │ Partition 1        │\n│ temps[0:50000]     │       │ temps[50000:100000]│\n│ humid[0:50000]     │       │ humid[50000:100000]│\n└────────────────────┘       └────────────────────┘\n         │                            │\n         └──────────┬─────────────────┘\n                    ▼\n        Distributed Reduction\n        (sum, mean, min, max, std)\n        → combines partial results\n          from all localities\n```\n\n### Key API\n\n| Function | Description |\n|----------|-------------|\n| `partitioned_from_numpy(arr)` | Create distributed array from NumPy |\n| `partitioned_arange(start, stop)` | Create distributed range |\n| `partitioned_zeros(shape)` | Create distributed zeros |\n| `distributed_sum(arr)` | Sum across all localities |\n| `distributed_mean(arr)` | Mean across all localities |\n| `distributed_min(arr)` | Min across all localities |\n| `distributed_max(arr)` | Max across all localities |\n| `distributed_std(arr)` | Std dev across all localities |\n| `distributed_var(arr)` | Variance across all localities |\n\n### Single vs Multi-Locality\n\n- **Single locality**: PartitionedArray behaves like a regular array\n- **Multi-locality**: Data is physically split across processes, reductions combine automatically",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nos.remove(\"_distributed_analytics_worker.py\")\nprint(\"Cleaned up worker script.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}