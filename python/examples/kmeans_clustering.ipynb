{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPXPy K-Means Clustering Demo\n",
    "\n",
    "This notebook implements K-Means clustering using HPXPy, demonstrating:\n",
    "1. Iterative MapReduce pattern (assign points -> update centroids)\n",
    "2. Data-parallel operations across large datasets\n",
    "3. How computation naturally partitions for distributed execution\n",
    "\n",
    "K-Means is a classic distributed computing benchmark because:\n",
    "- Data can be partitioned across localities (each owns a subset of points)\n",
    "- Each iteration has a Map phase (local) and Reduce phase (global)\n",
    "- Communication is minimal: only centroid updates need synchronization\n",
    "\n",
    "**Note:** For proper scalability testing with multiple thread counts, run the `kmeans_clustering_demo.py` script instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "hpx.init(num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Clustered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 500_000\n",
    "n_clusters = 10\n",
    "n_iterations = 20\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data points: {n_points:,}\")\n",
    "print(f\"  Clusters: {n_clusters}\")\n",
    "print(f\"  Iterations: {n_iterations}\")\n",
    "\n",
    "# Generate clustered data\n",
    "np.random.seed(42)\n",
    "\n",
    "points_per_cluster = n_points // n_clusters\n",
    "data_list = []\n",
    "for i in range(n_clusters):\n",
    "    center = np.random.randn(2) * 10\n",
    "    cluster_points = center + np.random.randn(points_per_cluster, 2)\n",
    "    data_list.append(cluster_points)\n",
    "\n",
    "data_np = np.vstack(data_list).astype(np.float64)\n",
    "np.random.shuffle(data_np)\n",
    "\n",
    "print(f\"\\nGenerated {len(data_np):,} 2D data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with HPXPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "x_np = data_np[:, 0].copy()\n",
    "y_np = data_np[:, 1].copy()\n",
    "\n",
    "# Convert to HPXPy arrays\n",
    "x = hpx.from_numpy(x_np)\n",
    "y = hpx.from_numpy(y_np)\n",
    "\n",
    "# Initialize centroids (random points from data)\n",
    "np.random.seed(123)\n",
    "centroid_idx = np.random.choice(n_points, n_clusters, replace=False)\n",
    "centroids_x = x_np[centroid_idx].copy()\n",
    "centroids_y = y_np[centroid_idx].copy()\n",
    "\n",
    "# Warm up\n",
    "_ = hpx.sum(x)\n",
    "\n",
    "# Time K-Means iterations\n",
    "start = time.perf_counter()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # === MAP PHASE: Assign each point to nearest centroid ===\n",
    "    min_distances = None\n",
    "    assignments_np = None\n",
    "    \n",
    "    for k in range(n_clusters):\n",
    "        # Distance squared: (x - cx)^2 + (y - cy)^2\n",
    "        dx = x - centroids_x[k]\n",
    "        dy = y - centroids_y[k]\n",
    "        dist_sq = dx * dx + dy * dy\n",
    "        \n",
    "        if min_distances is None:\n",
    "            min_distances = dist_sq\n",
    "            assignments_np = np.zeros(n_points, dtype=np.float64)\n",
    "        else:\n",
    "            # Update assignments where this centroid is closer\n",
    "            dist_sq_np = dist_sq.to_numpy()\n",
    "            min_dist_np = min_distances.to_numpy()\n",
    "            closer = dist_sq_np < min_dist_np\n",
    "            assignments_np[closer] = k\n",
    "            min_distances = hpx.from_numpy(np.minimum(min_dist_np, dist_sq_np))\n",
    "    \n",
    "    # === REDUCE PHASE: Update centroids ===\n",
    "    for k in range(n_clusters):\n",
    "        # Create mask for points in cluster k\n",
    "        mask = (assignments_np == k).astype(np.float64)\n",
    "        mask_hpx = hpx.from_numpy(mask)\n",
    "        \n",
    "        # Sum of coordinates for points in this cluster\n",
    "        sum_x = float(hpx.sum(x * mask_hpx))\n",
    "        sum_y = float(hpx.sum(y * mask_hpx))\n",
    "        count = float(hpx.sum(mask_hpx))\n",
    "        \n",
    "        if count > 0:\n",
    "            centroids_x[k] = sum_x / count\n",
    "            centroids_y[k] = sum_y / count\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "# Compute final inertia (sum of squared distances to centroids)\n",
    "total_inertia = float(hpx.sum(min_distances))\n",
    "\n",
    "print(f\"\\nK-Means Results:\")\n",
    "print(f\"  Time: {elapsed*1000:.2f} ms\")\n",
    "print(f\"  Inertia: {total_inertia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_kmeans(data, n_clusters, n_iterations):\n",
    "    \"\"\"NumPy reference K-Means implementation.\"\"\"\n",
    "    n_points = len(data)\n",
    "    x, y = data[:, 0], data[:, 1]\n",
    "    \n",
    "    # Initialize centroids\n",
    "    np.random.seed(123)\n",
    "    centroid_idx = np.random.choice(n_points, n_clusters, replace=False)\n",
    "    centroids_x = data[centroid_idx, 0].copy()\n",
    "    centroids_y = data[centroid_idx, 1].copy()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Compute distances to all centroids\n",
    "        distances = np.zeros((n_points, n_clusters))\n",
    "        for k in range(n_clusters):\n",
    "            distances[:, k] = (x - centroids_x[k])**2 + (y - centroids_y[k])**2\n",
    "        \n",
    "        # Assign to nearest centroid\n",
    "        assignments = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Update centroids\n",
    "        for k in range(n_clusters):\n",
    "            mask = assignments == k\n",
    "            if np.sum(mask) > 0:\n",
    "                centroids_x[k] = np.mean(x[mask])\n",
    "                centroids_y[k] = np.mean(y[mask])\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    # Compute inertia\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "    inertia = np.sum(min_distances)\n",
    "    \n",
    "    return elapsed, inertia\n",
    "\n",
    "np_time, np_inertia = numpy_kmeans(data_np, n_clusters, n_iterations)\n",
    "\n",
    "print(f\"NumPy K-Means:\")\n",
    "print(f\"  Time: {np_time*1000:.2f} ms\")\n",
    "print(f\"  Inertia: {np_inertia:.2f}\")\n",
    "print(f\"\\nSpeedup: {np_time/elapsed:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Subsample for plotting\n",
    "    sample_size = min(5000, n_points)\n",
    "    indices = np.random.choice(n_points, sample_size, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Color by assignment\n",
    "    plt.scatter(data_np[indices, 0], data_np[indices, 1], \n",
    "                c=assignments_np[indices], cmap='tab10', s=1, alpha=0.5)\n",
    "    \n",
    "    # Plot centroids\n",
    "    plt.scatter(centroids_x, centroids_y, c='black', marker='x', s=200, linewidths=3)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'K-Means Clustering ({n_clusters} clusters, {n_points:,} points)')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed K-Means: How It Scales Across Nodes\n",
    "\n",
    "K-Means has a natural MapReduce structure perfect for distribution:\n",
    "\n",
    "### Iteration Structure\n",
    "\n",
    "**MAP PHASE (Local - No Communication)**\n",
    "- Each locality processes its local data points\n",
    "- Compute distance from each point to all K centroids\n",
    "- Assign each point to nearest centroid\n",
    "- Compute local partial sums: Σx, Σy, count per cluster\n",
    "\n",
    "**REDUCE PHASE (Global - Minimal Communication)**\n",
    "- All-reduce to combine partial sums\n",
    "- Communication: Only 3×K floats per locality per iteration!\n",
    "\n",
    "### Scaling Projection\n",
    "\n",
    "| Localities | Points/Node | Communication | Expected Speedup |\n",
    "|------------|-------------|---------------|------------------|\n",
    "| 1 | 1,000,000 | 0 | 1x |\n",
    "| 4 | 250,000 | 120 floats | ~4x |\n",
    "| 16 | 62,500 | 480 floats | ~16x |\n",
    "| 64 | 15,625 | 1920 floats | ~60x |\n",
    "| 256 | 3,906 | 7680 floats | ~200x |\n",
    "\n",
    "Communication overhead is O(K) per iteration, independent of data size! This makes K-Means near-perfectly scalable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx.finalize()\n",
    "print(\"Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
