{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Advanced HPX Algorithms\n",
    "\n",
    "HPXPy provides direct access to HPX's powerful parallel algorithms, giving you fine-grained control over execution policies and advanced operations.\n",
    "\n",
    "This tutorial covers:\n",
    "- Scan algorithms (inclusive/exclusive scan)\n",
    "- Transform-reduce operations\n",
    "- Set operations (union, intersection, difference)\n",
    "- Selection algorithms (nth_element, median, percentile)\n",
    "- Search algorithms (searchsorted, isin, includes)\n",
    "- Grouped reductions (reduce_by_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpxpy as hpx\n",
    "import numpy as np\n",
    "\n",
    "# Initialize HPX runtime\n",
    "hpx.init()\n",
    "print(f\"Running with {hpx.num_threads()} threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scan-header",
   "metadata": {},
   "source": [
    "## 1. Scan Algorithms\n",
    "\n",
    "Scan algorithms compute running accumulations across an array. HPXPy exposes both inclusive and exclusive scan with custom operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-scan",
   "metadata": {},
   "outputs": [],
   "source": "# Inclusive scan: result[i] = op(arr[0], ..., arr[i])\narr = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n\nprint(\"Original:\", arr.to_numpy())\nprint(\"Inclusive scan (add):\", hpx.inclusive_scan(arr, \"add\").to_numpy())  # cumsum\nprint(\"Inclusive scan (mul):\", hpx.inclusive_scan(arr, \"mul\").to_numpy())  # cumprod"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-scan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusive scan: result[i] = op(init, arr[0], ..., arr[i-1])\n",
    "arr = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(\"Original:\", arr.to_numpy())\n",
    "print(\"Exclusive scan (add, init=0):\", hpx.exclusive_scan(arr, 0, \"add\").to_numpy())\n",
    "print(\"Exclusive scan (add, init=10):\", hpx.exclusive_scan(arr, 10, \"add\").to_numpy())\n",
    "print(\"Exclusive scan (mul, init=1):\", hpx.exclusive_scan(arr, 1, \"mul\").to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transform-reduce-header",
   "metadata": {},
   "source": [
    "## 2. Transform-Reduce\n",
    "\n",
    "The `transform_reduce` operation fuses a transform and reduction into a single pass over the data, which is more efficient than doing them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transform-reduce",
   "metadata": {},
   "outputs": [],
   "source": "arr = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n\nprint(\"Array:\", arr.to_numpy())\n\n# Sum of squares in one pass\nsum_sq = hpx.transform_reduce(arr, \"square\", \"add\")\nprint(f\"Sum of squares: {sum_sq}\")\nprint(f\"  Verification: {np.sum(arr.to_numpy()**2)}\")\n\n# Sum of absolute values\narr_signed = hpx.array([-1.0, 2.0, -3.0, 4.0, -5.0])\nsum_abs = hpx.transform_reduce(arr_signed, \"abs\", \"add\")\nprint(f\"\\nSigned array: {arr_signed.to_numpy()}\")\nprint(f\"Sum of absolute values: {sum_abs}\")\n\n# Product using identity transform\nproduct = hpx.transform_reduce(arr, \"identity\", \"mul\")\nprint(f\"\\nProduct of elements: {product}\")"
  },
  {
   "cell_type": "markdown",
   "id": "reduce-by-key-header",
   "metadata": {},
   "source": [
    "## 3. Reduce by Key\n",
    "\n",
    "Group values by key and reduce each group. This is useful for computing grouped statistics, histograms, and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduce-by-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grouped sum\n",
    "keys = hpx.array([1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 3.0])\n",
    "values = hpx.array([10.0, 20.0, 30.0, 40.0, 50.0, 5.0, 15.0])\n",
    "\n",
    "print(\"Keys:\", keys.to_numpy())\n",
    "print(\"Values:\", values.to_numpy())\n",
    "\n",
    "unique_keys, sums = hpx.reduce_by_key(keys, values, \"add\")\n",
    "print(\"\\nGrouped sums:\")\n",
    "for k, v in zip(unique_keys.to_numpy(), sums.to_numpy()):\n",
    "    print(f\"  Key {int(k)}: sum = {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduce-by-key-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different reduction operations\n",
    "keys = hpx.array([1.0, 1.0, 1.0, 2.0, 2.0, 2.0])\n",
    "values = hpx.array([3.0, 1.0, 4.0, 1.0, 5.0, 9.0])\n",
    "\n",
    "print(\"Keys:\", keys.to_numpy())\n",
    "print(\"Values:\", values.to_numpy())\n",
    "\n",
    "_, group_max = hpx.reduce_by_key(keys, values, \"max\")\n",
    "_, group_min = hpx.reduce_by_key(keys, values, \"min\")\n",
    "_, group_sum = hpx.reduce_by_key(keys, values, \"add\")\n",
    "\n",
    "print(\"\\nGroup 1: max={}, min={}, sum={}\".format(\n",
    "    group_max.to_numpy()[0], group_min.to_numpy()[0], group_sum.to_numpy()[0]))\n",
    "print(\"Group 2: max={}, min={}, sum={}\".format(\n",
    "    group_max.to_numpy()[1], group_min.to_numpy()[1], group_sum.to_numpy()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set-ops-header",
   "metadata": {},
   "source": [
    "## 4. Set Operations\n",
    "\n",
    "HPXPy provides efficient parallel set operations on sorted arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "b = hpx.array([3.0, 4.0, 5.0, 6.0, 7.0])\n",
    "\n",
    "print(\"a:\", a.to_numpy())\n",
    "print(\"b:\", b.to_numpy())\n",
    "print()\n",
    "\n",
    "# Union: elements in either set\n",
    "print(\"union1d(a, b):\", hpx.union1d(a, b).to_numpy())\n",
    "\n",
    "# Intersection: elements in both sets\n",
    "print(\"intersect1d(a, b):\", hpx.intersect1d(a, b).to_numpy())\n",
    "\n",
    "# Difference: elements in a but not in b\n",
    "print(\"setdiff1d(a, b):\", hpx.setdiff1d(a, b).to_numpy())\n",
    "\n",
    "# Symmetric difference: elements in exactly one set\n",
    "print(\"setxor1d(a, b):\", hpx.setxor1d(a, b).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-includes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if one set contains all elements of another\n",
    "superset = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "subset1 = hpx.array([2.0, 4.0])\n",
    "subset2 = hpx.array([2.0, 6.0])  # 6 not in superset\n",
    "\n",
    "print(\"Superset:\", superset.to_numpy())\n",
    "print(\"Subset 1:\", subset1.to_numpy())\n",
    "print(\"Subset 2:\", subset2.to_numpy())\n",
    "print()\n",
    "print(f\"Superset includes subset1? {hpx.includes(superset, subset1)}\")\n",
    "print(f\"Superset includes subset2? {hpx.includes(superset, subset2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-header",
   "metadata": {},
   "source": [
    "## 5. Selection Algorithms\n",
    "\n",
    "Selection algorithms find specific order statistics (like median, percentiles) in O(n) time, faster than sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nth-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nth_element: Find the nth smallest element in O(n) time\n",
    "arr = hpx.array([7.0, 2.0, 9.0, 1.0, 5.0, 8.0, 3.0, 6.0, 4.0])\n",
    "\n",
    "print(\"Array:\", arr.to_numpy())\n",
    "print(\"Sorted would be: [1, 2, 3, 4, 5, 6, 7, 8, 9]\")\n",
    "print()\n",
    "print(f\"nth_element(arr, 0) = {hpx.nth_element(arr, 0)} (minimum)\")\n",
    "print(f\"nth_element(arr, 4) = {hpx.nth_element(arr, 4)} (5th smallest = median)\")\n",
    "print(f\"nth_element(arr, -1) = {hpx.nth_element(arr, -1)} (maximum)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-percentile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median and percentiles\n",
    "arr = hpx.arange(100, dtype='float64')\n",
    "\n",
    "print(f\"Median: {hpx.median(arr)}\")\n",
    "print(f\"25th percentile: {hpx.percentile(arr, 25)}\")\n",
    "print(f\"75th percentile: {hpx.percentile(arr, 75)}\")\n",
    "print(f\"90th percentile: {hpx.percentile(arr, 90)}\")\n",
    "\n",
    "# Verify against NumPy\n",
    "np_arr = arr.to_numpy()\n",
    "print(f\"\\nNumPy median: {np.median(np_arr)}\")\n",
    "print(f\"NumPy 25th percentile: {np.percentile(np_arr, 25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-header",
   "metadata": {},
   "source": [
    "## 6. Search Algorithms\n",
    "\n",
    "Efficient algorithms for searching sorted arrays and testing membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searchsorted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchsorted: Binary search for insertion points\n",
    "sorted_arr = hpx.array([10.0, 20.0, 30.0, 40.0, 50.0])\n",
    "values = hpx.array([15.0, 25.0, 35.0, 5.0, 55.0])\n",
    "\n",
    "print(\"Sorted array:\", sorted_arr.to_numpy())\n",
    "print(\"Values to search:\", values.to_numpy())\n",
    "\n",
    "indices = hpx.searchsorted(sorted_arr, values)\n",
    "print(\"\\nInsertion indices (left):\")\n",
    "for v, i in zip(values.to_numpy(), indices.to_numpy()):\n",
    "    print(f\"  {v} -> index {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isin: Test membership\n",
    "arr = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n",
    "test_values = hpx.array([2.0, 4.0, 6.0, 8.0])  # Even numbers\n",
    "\n",
    "print(\"Array:\", arr.to_numpy())\n",
    "print(\"Test values:\", test_values.to_numpy())\n",
    "\n",
    "mask = hpx.isin(arr, test_values)\n",
    "print(\"Is in test values:\", mask.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonzero",
   "metadata": {},
   "outputs": [],
   "source": "# nonzero: Find indices of non-zero elements\narr = hpx.array([0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 3.0, 0.0])\n\nprint(\"Array:\", arr.to_numpy())\nnz_indices = hpx.nonzero(arr)\nprint(\"Non-zero indices:\", nz_indices.to_numpy())\n\n# Use NumPy for fancy indexing (not yet supported in HPXPy)\nnp_arr = arr.to_numpy()\nprint(\"Non-zero values:\", np_arr[nz_indices.to_numpy()])"
  },
  {
   "cell_type": "markdown",
   "id": "array-ops-header",
   "metadata": {},
   "source": [
    "## 7. Array Manipulation\n",
    "\n",
    "Additional algorithms for manipulating and comparing arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flip-roll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip: Reverse array\n",
    "arr = hpx.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(\"Original:\", arr.to_numpy())\n",
    "print(\"Flipped:\", hpx.flip(arr).to_numpy())\n",
    "\n",
    "# roll: Rotate array elements\n",
    "print(\"\\nRoll examples:\")\n",
    "print(\"Roll by 2 (left):\", hpx.roll(arr, 2).to_numpy())\n",
    "print(\"Roll by -2 (right):\", hpx.roll(arr, -2).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "array-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_equal: Compare arrays\n",
    "a = hpx.array([1.0, 2.0, 3.0])\n",
    "b = hpx.array([1.0, 2.0, 3.0])\n",
    "c = hpx.array([1.0, 2.0, 4.0])\n",
    "\n",
    "print(\"a:\", a.to_numpy())\n",
    "print(\"b:\", b.to_numpy())\n",
    "print(\"c:\", c.to_numpy())\n",
    "print()\n",
    "print(f\"array_equal(a, b): {hpx.array_equal(a, b)}\")\n",
    "print(f\"array_equal(a, c): {hpx.array_equal(a, c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-sorted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_sorted: Merge two sorted arrays\n",
    "sorted1 = hpx.array([1.0, 3.0, 5.0, 7.0, 9.0])\n",
    "sorted2 = hpx.array([2.0, 4.0, 6.0, 8.0, 10.0])\n",
    "\n",
    "print(\"Sorted array 1:\", sorted1.to_numpy())\n",
    "print(\"Sorted array 2:\", sorted2.to_numpy())\n",
    "print(\"Merged:\", hpx.merge_sorted(sorted1, sorted2).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-sort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable_sort: Preserves relative order of equal elements\n",
    "arr = hpx.array([3.0, 1.0, 4.0, 1.0, 5.0, 9.0, 2.0, 6.0])\n",
    "\n",
    "print(\"Original:\", arr.to_numpy())\n",
    "print(\"Stable sorted:\", hpx.stable_sort(arr).to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deterministic-header",
   "metadata": {},
   "source": [
    "## 8. Deterministic Reductions\n",
    "\n",
    "Parallel floating-point reductions can produce different results due to non-associativity. Use `reduce_deterministic` when reproducibility is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deterministic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate potential floating-point variance\n",
    "arr = hpx.array([1e20, 1.0, -1e20, 2.0, 1e20, 3.0, -1e20])\n",
    "\n",
    "print(\"Array with large magnitude differences:\")\n",
    "print(arr.to_numpy())\n",
    "print()\n",
    "\n",
    "# Parallel reduction (may vary)\n",
    "par_result = hpx.reduce(arr, \"add\", policy=\"par\")\n",
    "print(f\"Parallel sum: {par_result}\")\n",
    "\n",
    "# Deterministic reduction (always same result)\n",
    "det_result = hpx.reduce_deterministic(arr, \"add\")\n",
    "print(f\"Deterministic sum: {det_result}\")\n",
    "\n",
    "# Compare to NumPy\n",
    "np_result = np.sum(arr.to_numpy())\n",
    "print(f\"NumPy sum: {np_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-header",
   "metadata": {},
   "source": [
    "## 9. Performance Example\n",
    "\n",
    "Let's benchmark some algorithms on larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n = 1_000_000\n",
    "arr = hpx.random.uniform(0, 1000, size=n)\n",
    "\n",
    "print(f\"Array size: {n:,} elements\")\n",
    "print()\n",
    "\n",
    "def benchmark(name, func):\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    print(f\"{name:25s}: {elapsed:7.2f} ms\")\n",
    "    return result\n",
    "\n",
    "# Benchmark various algorithms\n",
    "benchmark(\"sum (parallel)\", lambda: hpx.sum(arr, policy=\"par\"))\n",
    "benchmark(\"transform_reduce (sum sq)\", lambda: hpx.transform_reduce(arr, \"square\", \"add\"))\n",
    "benchmark(\"inclusive_scan (cumsum)\", lambda: hpx.inclusive_scan(arr, \"add\"))\n",
    "benchmark(\"median (O(n) selection)\", lambda: hpx.median(arr))\n",
    "benchmark(\"unique\", lambda: hpx.unique(arr))\n",
    "benchmark(\"sort (parallel)\", lambda: hpx.sort(arr, policy=\"par\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "hpx.finalize()\n",
    "print(\"Runtime finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about HPXPy's advanced algorithms:\n",
    "\n",
    "1. **Scan Algorithms**: `inclusive_scan`, `exclusive_scan` - running accumulations with custom operations\n",
    "2. **Transform-Reduce**: `transform_reduce` - fused transform + reduce for efficiency\n",
    "3. **Grouped Reductions**: `reduce_by_key` - aggregate values by group\n",
    "4. **Set Operations**: `union1d`, `intersect1d`, `setdiff1d`, `setxor1d`, `includes`\n",
    "5. **Selection Algorithms**: `nth_element`, `median`, `percentile` - O(n) order statistics\n",
    "6. **Search Algorithms**: `searchsorted`, `isin`, `nonzero`\n",
    "7. **Array Manipulation**: `flip`, `roll`, `array_equal`, `merge_sorted`, `stable_sort`, `partition`\n",
    "8. **Deterministic Reductions**: `reduce`, `reduce_deterministic` - control over reproducibility\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use **execution policies** (`policy=\"seq\"` or `policy=\"par\"`) to control parallelism\n",
    "- **Transform-reduce** is more efficient than separate transform + reduce\n",
    "- **Selection algorithms** (nth_element, median) are O(n), faster than sorting for single values\n",
    "- **Set operations** work on sorted arrays and are parallelized\n",
    "- Use **reduce_deterministic** when you need reproducible floating-point results\n",
    "\n",
    "These algorithms directly expose HPX's parallel capabilities, giving you fine-grained control over execution while maintaining NumPy-like ergonomics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}