{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Parallel Image Processing with HPXPy\n",
    "\n",
    "This notebook demonstrates parallel image processing operations using HPXPy.\n",
    "\n",
    "**Topics covered:**\n",
    "- Image normalization\n",
    "- Channel operations with broadcasting\n",
    "- Batch image statistics\n",
    "- Parallel convolution concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hpxpy as hpx\n",
    "\n",
    "# Initialize HPX runtime\n",
    "hpx.init()\n",
    "print(f\"HPXPy initialized with {hpx.num_threads()} threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-header",
   "metadata": {},
   "source": [
    "## 2. Creating Synthetic Images\n",
    "\n",
    "We'll create synthetic grayscale and RGB images to demonstrate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradient_image(height, width):\n",
    "    \"\"\"Create a horizontal gradient image.\"\"\"\n",
    "    x = np.linspace(0, 1, width, dtype=np.float64)\n",
    "    return np.tile(x, (height, 1))\n",
    "\n",
    "def create_checkerboard(height, width, square_size=8):\n",
    "    \"\"\"Create a checkerboard pattern.\"\"\"\n",
    "    x = np.arange(width) // square_size\n",
    "    y = np.arange(height) // square_size\n",
    "    return ((x[np.newaxis, :] + y[:, np.newaxis]) % 2).astype(np.float64)\n",
    "\n",
    "def create_circle_image(height, width, center=None, radius=None):\n",
    "    \"\"\"Create an image with a circle.\"\"\"\n",
    "    if center is None:\n",
    "        center = (height // 2, width // 2)\n",
    "    if radius is None:\n",
    "        radius = min(height, width) // 4\n",
    "    \n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    dist = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n",
    "    return (dist <= radius).astype(np.float64)\n",
    "\n",
    "# Create test images\n",
    "H, W = 256, 256\n",
    "\n",
    "gradient = create_gradient_image(H, W)\n",
    "checkerboard = create_checkerboard(H, W, 32)\n",
    "circle = create_circle_image(H, W)\n",
    "\n",
    "print(f\"Created images with shape: {gradient.shape}\")\n",
    "print(f\"Gradient range: [{gradient.min():.2f}, {gradient.max():.2f}]\")\n",
    "print(f\"Checkerboard unique values: {np.unique(checkerboard)}\")\n",
    "print(f\"Circle pixels: {circle.sum():.0f} (inside circle)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalize-header",
   "metadata": {},
   "source": [
    "## 3. Image Normalization\n",
    "\n",
    "Normalize images using parallel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image_numpy(img):\n",
    "    \"\"\"Normalize image to [0, 1] range.\"\"\"\n",
    "    img_min = img.min()\n",
    "    img_max = img.max()\n",
    "    if img_max - img_min > 0:\n",
    "        return (img - img_min) / (img_max - img_min)\n",
    "    return img - img_min\n",
    "\n",
    "def normalize_image_hpxpy(img):\n",
    "    \"\"\"Normalize image using HPXPy.\"\"\"\n",
    "    img_hpx = hpx.from_numpy(img) if not isinstance(img, hpx.ndarray) else img\n",
    "    \n",
    "    img_min = hpx.min(img_hpx)\n",
    "    img_max = hpx.max(img_hpx)\n",
    "    \n",
    "    if img_max - img_min > 0:\n",
    "        return (img_hpx - img_min) / (img_max - img_min)\n",
    "    return img_hpx - img_min\n",
    "\n",
    "# Test normalization\n",
    "# Create an unnormalized image\n",
    "unnorm = gradient * 200 + 50  # Range [50, 250]\n",
    "\n",
    "norm_np = normalize_image_numpy(unnorm)\n",
    "norm_hpx = normalize_image_hpxpy(unnorm)\n",
    "\n",
    "print(f\"Original range: [{unnorm.min():.0f}, {unnorm.max():.0f}]\")\n",
    "print(f\"NumPy normalized: [{norm_np.min():.2f}, {norm_np.max():.2f}]\")\n",
    "print(f\"HPXPy normalized: [{hpx.min(norm_hpx):.2f}, {hpx.max(norm_hpx):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rgb-header",
   "metadata": {},
   "source": [
    "## 4. RGB Channel Operations\n",
    "\n",
    "Broadcasting enables efficient per-channel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rgb-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple RGB image (H, W, 3)\n",
    "def create_rgb_image(height, width):\n",
    "    \"\"\"Create synthetic RGB image.\"\"\"\n",
    "    img = np.zeros((height, width, 3), dtype=np.float64)\n",
    "    img[:, :, 0] = create_gradient_image(height, width)      # Red: horizontal gradient\n",
    "    img[:, :, 1] = create_gradient_image(height, width).T[:height, :width]  # Green: vertical\n",
    "    img[:, :, 2] = create_circle_image(height, width) * 0.8  # Blue: circle\n",
    "    return img\n",
    "\n",
    "rgb = create_rgb_image(H, W)\n",
    "print(f\"RGB image shape: {rgb.shape}\")\n",
    "\n",
    "# Channel-wise statistics using broadcasting\n",
    "rgb_hpx = hpx.from_numpy(rgb)\n",
    "\n",
    "# Compute per-channel mean (would need axis support)\n",
    "# For now, compute overall stats\n",
    "print(f\"\\nChannel statistics:\")\n",
    "for c, name in enumerate(['Red', 'Green', 'Blue']):\n",
    "    channel = hpx.from_numpy(rgb[:, :, c])\n",
    "    print(f\"  {name}: mean={hpx.mean(channel):.3f}, std={hpx.std(channel):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "channel-adjust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust brightness/contrast per channel\n",
    "def adjust_brightness_contrast(img, brightness=0, contrast=1.0):\n",
    "    \"\"\"Adjust image brightness and contrast.\"\"\"\n",
    "    img_hpx = hpx.from_numpy(img) if not isinstance(img, hpx.ndarray) else img\n",
    "    \n",
    "    # Apply: out = (img - 0.5) * contrast + 0.5 + brightness\n",
    "    adjusted = (img_hpx - 0.5) * contrast + 0.5 + brightness\n",
    "    \n",
    "    # Clip to [0, 1] - convert to numpy for clipping\n",
    "    result = adjusted.to_numpy()\n",
    "    return np.clip(result, 0, 1)\n",
    "\n",
    "# Test adjustments\n",
    "original = gradient.copy()\n",
    "brightened = adjust_brightness_contrast(original, brightness=0.2)\n",
    "contrasted = adjust_brightness_contrast(original, contrast=1.5)\n",
    "\n",
    "print(f\"Original mean: {original.mean():.3f}\")\n",
    "print(f\"Brightened mean: {brightened.mean():.3f}\")\n",
    "print(f\"Contrasted std: {contrasted.std():.3f} vs original: {original.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-header",
   "metadata": {},
   "source": [
    "## 5. Batch Image Processing\n",
    "\n",
    "Process multiple images in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of images\n",
    "n_images = 100\n",
    "batch_h, batch_w = 64, 64\n",
    "\n",
    "# Create batch as (N, H, W) array\n",
    "batch = np.random.rand(n_images, batch_h, batch_w).astype(np.float64)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Total pixels: {batch.size:,}\")\n",
    "\n",
    "# Compute statistics for entire batch\n",
    "batch_hpx = hpx.from_numpy(batch.reshape(-1))  # Flatten for reduction\n",
    "\n",
    "print(f\"\\nBatch statistics:\")\n",
    "print(f\"  Mean: {hpx.mean(batch_hpx):.4f}\")\n",
    "print(f\"  Std:  {hpx.std(batch_hpx):.4f}\")\n",
    "print(f\"  Min:  {hpx.min(batch_hpx):.4f}\")\n",
    "print(f\"  Max:  {hpx.max(batch_hpx):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalize(images):\n",
    "    \"\"\"\n",
    "    Normalize a batch of images.\n",
    "    images: (N, H, W) array\n",
    "    \"\"\"\n",
    "    # Global normalization across entire batch\n",
    "    flat = hpx.from_numpy(images.reshape(-1))\n",
    "    \n",
    "    mean = hpx.mean(flat)\n",
    "    std = hpx.std(flat)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized = (flat - mean) / (std + 1e-8)\n",
    "    \n",
    "    return normalized.to_numpy().reshape(images.shape)\n",
    "\n",
    "# Normalize batch\n",
    "start = time.perf_counter()\n",
    "normalized_batch = batch_normalize(batch)\n",
    "elapsed = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"Normalized {n_images} images in {elapsed:.2f} ms\")\n",
    "print(f\"Normalized mean: {normalized_batch.mean():.6f}\")\n",
    "print(f\"Normalized std:  {normalized_batch.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perf-header",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_image_ops(n_images, img_size, warmup=3, repeats=5):\n",
    "    \"\"\"Benchmark image processing operations.\"\"\"\n",
    "    # Create batch\n",
    "    batch = np.random.rand(n_images, img_size, img_size).astype(np.float64)\n",
    "    flat = batch.reshape(-1)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = np.mean(flat)\n",
    "        _ = hpx.mean(hpx.from_numpy(flat))\n",
    "    \n",
    "    # Time NumPy\n",
    "    np_times = []\n",
    "    for _ in range(repeats):\n",
    "        start = time.perf_counter()\n",
    "        _ = np.mean(flat)\n",
    "        _ = np.std(flat)\n",
    "        normalized = (flat - flat.mean()) / (flat.std() + 1e-8)\n",
    "        np_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    # Time HPXPy\n",
    "    hpx_times = []\n",
    "    flat_hpx = hpx.from_numpy(flat)\n",
    "    for _ in range(repeats):\n",
    "        start = time.perf_counter()\n",
    "        mean = hpx.mean(flat_hpx)\n",
    "        std = hpx.std(flat_hpx)\n",
    "        normalized = (flat_hpx - mean) / (std + 1e-8)\n",
    "        hpx_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return min(np_times) * 1000, min(hpx_times) * 1000\n",
    "\n",
    "print(f\"Image Processing Performance (HPX threads: {hpx.num_threads()})\")\n",
    "print(f\"{'Images':>8} {'Size':>8} {'Pixels':>12} {'NumPy (ms)':>12} {'HPXPy (ms)':>12} {'Speedup':>10}\")\n",
    "print(\"=\" * 68)\n",
    "\n",
    "configs = [\n",
    "    (10, 256),\n",
    "    (100, 128),\n",
    "    (100, 256),\n",
    "    (1000, 64),\n",
    "]\n",
    "\n",
    "for n_img, size in configs:\n",
    "    pixels = n_img * size * size\n",
    "    np_time, hpx_time = benchmark_image_ops(n_img, size)\n",
    "    speedup = np_time / hpx_time if hpx_time > 0 else float('inf')\n",
    "    print(f\"{n_img:>8} {size:>8} {pixels:>12,} {np_time:>12.2f} {hpx_time:>12.2f} {speedup:>9.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx.finalize()\n",
    "print(\"HPX runtime finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "HPXPy enables parallel image processing through:\n",
    "\n",
    "1. **Parallel reductions** - `mean`, `std`, `min`, `max` on large images\n",
    "2. **Element-wise operations** - Brightness, contrast, normalization\n",
    "3. **Broadcasting** - Per-channel operations efficiently\n",
    "4. **Batch processing** - Process many images simultaneously\n",
    "\n",
    "### Future Extensions\n",
    "\n",
    "With additional HPXPy features, we could add:\n",
    "- Convolution filters (requires strided operations)\n",
    "- FFT-based filtering (requires FFT)\n",
    "- PCA/Eigenfaces (requires linear algebra)\n",
    "- GPU-accelerated processing\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Digital Image Processing](https://en.wikipedia.org/wiki/Digital_image_processing)\n",
    "- [Image Normalization](https://en.wikipedia.org/wiki/Normalization_(image_processing))\n",
    "- [Eigenfaces](https://en.wikipedia.org/wiki/Eigenface)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
